import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, TimeDistributed, Lambda
)
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler


# Split data to prevent leakage
train_size = int(len(df) * 0.8)
train_data = df.iloc[:train_size]
test_data = df.iloc[train_size:]

# Scale separately to avoid data leakage
scaler = MinMaxScaler()
scaled_train = scaler.fit_transform(train_data)
scaled_test = scaler.transform(test_data)

# Function to create sequences with step size = seq_length
def create_sequences(data, seq_length):
    X, decoder_inputs, decoder_targets = [], [], []

    for i in range(0, len(data) - seq_length, seq_length):  # Step size = seq_length
        X.append(data[i:i+seq_length])  # Encoder input: Full sequence

        # Decoder input: Start token + shifted sequence
        start_token = np.zeros((1, data.shape[1]))  # Start token (1 row of zeros)
        decoder_in = np.vstack([start_token, data[i+1:i+seq_length]])

        # Decoder target: Shifted sequence
        decoder_target = data[i+1:i+seq_length+1]

        decoder_inputs.append(decoder_in)
        decoder_targets.append(decoder_target)

    return np.array(X), np.array(decoder_inputs), np.array(decoder_targets)

seq_length = 10  # Look at last 10 time steps
X_train, decoder_inputs_train, decoder_targets_train = create_sequences(scaled_train, seq_length)
X_test, decoder_inputs_test, decoder_targets_test = create_sequences(scaled_test, seq_length)

# Transformer Encoder Block
def transformer_encoder(inputs, num_heads=4, ff_dim=64, dropout_rate=0.1):
    input_dim = tf.keras.backend.int_shape(inputs)[-1]
    key_dim = max(1, input_dim // num_heads)

    attention_norm = LayerNormalization(epsilon=1e-6)(inputs)
    attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(attention_norm, attention_norm)
    attention = Dropout(dropout_rate)(attention)
    attention = inputs + attention
    attention = LayerNormalization(epsilon=1e-6)(attention)

    ff_output = Dense(ff_dim, activation="relu")(attention)
    ff_output = Dense(input_dim)(ff_output)
    ff_output = Dropout(dropout_rate)(ff_output)

    return LayerNormalization(epsilon=1e-6)(attention + ff_output)

# Transformer Decoder Block with Causal Mask
def transformer_decoder(inputs, encoder_outputs, num_heads=4, ff_dim=64, dropout_rate=0.1):
    input_dim = tf.keras.backend.int_shape(inputs)[-1]
    key_dim = max(1, input_dim // num_heads)

    # Define causal mask using Lambda layer
    def create_causal_mask(seq_len):
        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # Lower triangular mask
        return 1 - mask  # Invert: 1 where to mask, 0 where allowed

    causal_mask = Lambda(lambda x: create_causal_mask(tf.shape(x)[1]))(inputs)

    attention_norm = LayerNormalization(epsilon=1e-6)(inputs)
    attention1 = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(
        attention_norm, attention_norm, attention_mask=causal_mask
    )
    attention1 = Dropout(dropout_rate)(attention1)
    attention1 = inputs + attention1
    attention1 = LayerNormalization(epsilon=1e-6)(attention1)

    attention2 = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(
        query=attention1, key=encoder_outputs, value=encoder_outputs
    )
    attention2 = Dropout(dropout_rate)(attention2)
    attention2 = attention1 + attention2
    attention2 = LayerNormalization(epsilon=1e-6)(attention2)

    ff_output = Dense(ff_dim, activation="relu")(attention2)
    ff_output = Dense(input_dim)(ff_output)
    ff_output = Dropout(dropout_rate)(ff_output)

    return LayerNormalization(epsilon=1e-6)(attention2 + ff_output)

# Build Transformer Model
def build_transformer_model(input_shape, target_shape, num_heads=4, ff_dim=64):
    encoder_inputs = Input(shape=input_shape, name="encoder_inputs")
    encoder_outputs = transformer_encoder(encoder_inputs, num_heads, ff_dim)
    encoder_outputs = transformer_encoder(encoder_outputs, num_heads, ff_dim)

    decoder_inputs = Input(shape=target_shape, name="decoder_inputs")
    decoder_outputs = transformer_decoder(decoder_inputs, encoder_outputs, num_heads, ff_dim)
    decoder_outputs = transformer_decoder(decoder_outputs, encoder_outputs, num_heads, ff_dim)

    # Preserve Temporal Structure with TimeDistributed
    x = TimeDistributed(Dense(32, activation="relu"))(decoder_outputs)
    outputs = TimeDistributed(Dense(target_shape[-1]))(x)  # Output per time step

    model = Model([encoder_inputs, decoder_inputs], outputs)
    return model

# Define target shape correctly
target_shape = (seq_length, X_train.shape[-1])  # Adjusted to match correct sequence length

# Build model
model = build_transformer_model((seq_length, X_train.shape[2]), target_shape, num_heads=4, ff_dim=64)

# Compile Model
model.compile(optimizer=Adam(learning_rate=0.001), loss="mse", metrics=['accuracy'])

# Train Model
history = model.fit(
    [X_train, decoder_inputs_train], decoder_targets_train,
    epochs=50, batch_size=16,
    validation_data=([X_test, decoder_inputs_test], decoder_targets_test)
)

# Plot Training History
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title("Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()
